---
title: Combining (lots of) numerical and categorical predictors
subtitle: ANCOVAs and beyond
bibliography: ../references.bib
---

<!-- COMMENT NOT SHOW IN ANY OUTPUT: Code chunk below sets overall defaults for .qmd file; these inlcude showing output by default and looking for files relative to .Rpoj file, not .qmd file, which makes putting filesin different folders easier  -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
source("../globals.R")

```

So far we've used linear models to consider how categorical predictors
and continuous numerical predictors can predict continuous outcomes.
We've already considered having multiple categorical predictors
(remember factorial ANOVAs). Now we'll extend those ideas to models
where we have numerous categorical and/or continuous numerical
predictors in the same model. In doing so we'll introduce (and connect)
ANCOVAs, multiple regression, and model selection.

## Back to the iris data

We will motivate this (again!) with an example from our favorite iris
data. So far we have considered how species impacts sepal lengths
(abbreviated analysis:

```{r}
library(Rmisc)
function_output <- summarySE(iris, measurevar="Sepal.Length", groupvars =
                               c("Species"))

library(ggplot2)
ggplot(function_output, aes(y=Sepal.Length, x=Species, fill=Species)) +
  geom_col(aes(fill=Species)) +
    geom_errorbar(aes(ymin=Sepal.Length-ci, ymax=Sepal.Length+ci)) +
  labs(title=expression(paste("Sepal lengths of ",italic("I. species"))),
       y= "Sepal length (cm)",
       x= "Species")
```

```{r}
iris_anova <- lm(Sepal.Length~Species, iris)
plot(iris_anova)
library(car)
Anova(iris_anova, type="III")
library(multcomp)
compare_cont_tukey <- glht(iris_anova, linfct = mcp(Species = "Tukey"))
summary(compare_cont_tukey)
```

We've also considered how petal length influences sepal length
(abbreviated analysis:

```{r}
ggplot(iris, aes(y=Sepal.Length, x=Petal.Length)) +
  geom_point() +
  labs(title="Sepal length increases with petal length",
       y= "Sepal length (cm)",
       x= "Petal Length (cm)") +
  geom_smooth(method = "lm",se = F)

```

```{r}
iris_regression <- lm(Sepal.Length ~ Petal.Length, iris)
plot(iris_regression)
Anova(iris_regression, type = "III")
summary(iris_regression)
```

However, what if we wanted to consider the combined (and potentially
interacting) effects of petal length and species on sepal length? This
is like our 2-way ANOVA but with one predictor being continuous. This is
often called an ANCOVA, but it's just another linear model! Overall, we
are decomposing the variance of each data point among various factors.
Our use of type III residuals let's us ask how much any given factor
explains *given* that other factors are in the model (we'll explore this
more below). Put another way, we might want to know if a factor adds
explanatory power to the model (and is not redundant or subsumed by
another factor).

Continuing to use focal iris dataset, we can use the same format we used
for factorial or 2-way ANOVAs to add the factors.

```{r}
iris_ancova <- lm(Sepal.Length~Species*Petal.Length, iris)
```

This model includes an interaction and matche the following null
hypotheses (note 2 are copied from previous sections!):

$$
\begin{split}
H_O: \mu_{sepal \ length, \ setosa} = \mu_{sepal \ length, \ virginica} = \mu_{sepal \ length, \ versicolor}\\ 
H_O: \beta_\textrm{(coefficient between sepal and petal length)} = 0\\\\
H_O: \textrm{relationship between petal length and sepal length does not differ
among species}\\
\end{split}
$$

Once we develop the model, we can visually check the assumptions, which
remain

$$
\epsilon \approx i.i.d.\ N(\mu,\sigma)
$$

```{r}
plot(iris_ancova)
```

If assumptions appear to be met (as they do here), we can consider
impacts of factors. Given the presence of categorical predictors, an
Anova table may be informative

```{r}
Anova(iris_ancova, type="III")
```

Since the interaction is not significant (F~2,144~ = 1.68, p = 0.19), we
have the same 2 options we noted in factorial ANOVAs.

### Interpret results with the interaction

We can read results from the full model. These show no interaction
between species and petal length (F~2,144~ = 1.68, p = 0.1895) and no
impact of petal length (F~1,144~=3.83, p = 0.052) but a significant
impact of species (F~2,144~ = 12.81, p \< 0.01). We can follow this up
with a post-hoc test

```{r}
compare_ancova_tukey <- glht(iris_ancova, linfct = mcp(Species = "Tukey"))
summary(compare_ancova_tukey)
```

which suggests *I. versicolor* and *I. virginica* are similar to each
other but different from other species.

However, note the *glht* output notes interactions are still present in
model, so this approach may be inappropriate. For ANOVA's we noted could
instead compare means for each combination.

```{r}
library(emmeans)
emmeans(iris_ancova, pairwise ~ Species*Petal.Length)
```

This approach, however, is less useful for numerical predictors. By
default the **emmeans** package uses the average for the covariate
(note,

```{r}
mean(iris$Petal.Length)
```

is why we see *3.578* spread throughout the output). While we can
specify other breaks, this highlights the difference in combining
cateorical variable impacts and combining effects of categorical and
continuous variables.

### Remove the interaction

Since the interaction isn't significant, we can also remove it.

```{r}
iris_ancova_updated <- lm(Sepal.Length~Species+Petal.Length, iris)
plot(iris_ancova_updated)
Anova(iris_ancova_updated, type="III")
```

This shows us that both petal length and species impact sepal length.

Why did these factors "suddenly" become significant. The issue lies in
that they are not fully independent. Note

```{r}
petal_anova <- lm(Petal.Length ~ Species, iris)
plot(petal_anova)
Anova(petal_anova, type ="III")
summary(petal_anova)
```

This shows that species explains a lot
(`r summary(petal_anova)$r.squared` %, in fact) of the variation in
petal length. Thus interactions among species and petal length are hard
to define, as each species petal lengths are *mostly* different than the
others.

This relates to an issue we will soon see. When you consider the impacts
of multiple factors on a variable, you are assuming they are not
related. That is rarely true (except for when we set up factorial
ANOVAs), so we have to decide how related is ok. As factors become more
related, the linear model approach does not work.On one level, its hard
to split variance correctly among two similar columns. Mathematically,
it also makes the design matrix harder to invert.

<details>

<summary>How does this relate to types of residuals?</summary>

This also relates to the "types" of residuals. Type 1 notes the order of
factors in the model. The function *anova* uses this type. Note the
values are different (but still significant) depending on the order.

```{r}
anova(iris_ancova)
```

and

```{r}
iris_ancova_reversed <- lm(Sepal.Length~Petal.Length*Species, iris)
anova(iris_ancova_reversed)
```

This is because Type 1 residuals remove all the variation that can be
explained by the first factor (even if it *could* be given to a related
factor), then do the same for the second and so forth. So related
factors will have different p-values depending on order (unbalanced
designs also impact this.)

Type II residuals don't focus on order, but they ignore interactions. In
doing so they ignore any variation that could be attributed to multiple
factors.

```{r}
Anova(iris_ancova, type="II")
```

Type III residuals include interactions, but ask how much a given factor
contributes to the explanatory power of a model given main effects of
other factors and interactions, including those with the focal factor,
are already present. This use of *marginal means* may seem odd (e.g.,
asking if a factor should be included when you are already including
interactions with said factor) and also means the sum of squares that we
get from decomposing the variance adds up to *more* than the total sum
of squares. However, from a conceptual standpoint they work and are thus
commonly used.

</details>

### What would interactions look like?

So, what would interactions actually look like, and how would you
interpret them? To illustrate this, let's pretend we visit another
valley and sample 3 new iris species (*I. baruch*, *I. hunter*, and *I.
york)*. We want to see how species and petal length impact sepal length
in these species. Let's make some data to suggest potential outcomes.

#### No impact of petal length or species

First, we might find no relationship between the variables. That might
looi something like this:

```{r, echo=F}
set.seed(3)
iris_example_species <-data.frame(
  Species = c(rep("baruch",25), rep("hunter", 25), rep("york", 25)),
  Petal_Length = runif(75,2,4 ),
  #no difference based on species or sepal length
  Sepal_no_impacts = runif (75, 4, 6))
iris_example_species$Species <- factor(iris_example_species$Species)
```

```{r}
ggplot(iris_example_species, aes(x= Petal_Length, y = Sepal_no_impacts, color = Species)) +
  geom_point()+
  ylab("Sepal Length") +
  xlab("Petal Length") +
  ggtitle("Impact of petal length and species on sepal length") +
    geom_smooth(method = "lm", se = F)

```

Analysis would indicate (assumption plots not shown here to allow focus
on interpreting interactions)

```{r}
Anova(lm( Sepal_no_impacts~ Petal_Length * Species, iris_example_species), 
      type = "III")
```

no impact of interaction, so we could drop it

```{r}
Anova(lm( Sepal_no_impacts~ Petal_Length + Species, iris_example_species), 
      type = "III")
```

but we still find no main effects.

#### Impact of petal length but not species

Alternatively, we might find a situation where there is a relationship
between petal length and sepal length, but no impact of species

```{r, echo = F}
set.seed(77)
iris_example_species$Sepal_no_impact_species <- 
  iris_example_species$Petal_Length * 2 + rnorm(75)
```

```{r}
ggplot(iris_example_species, aes(x= Petal_Length, y = Sepal_no_impact_species, color = Species)) +
  geom_point()+
  ylab("Sepal Length") +
  xlab("Petal Length") +
  ggtitle("Impact of petal length and species on sepal length") +
    geom_smooth(method = "lm", se = F)

```

Analysis would indicate (assumption plots not shown here to allow focus
on interpreting interactions)

```{r}
Anova(lm( Sepal_no_impact_species~ Petal_Length * Species, iris_example_species), 
      type = "III")
```

no impact of interaction, so we could drop it

```{r}
Anova(lm( Sepal_no_impact_species~ Petal_Length + Species, iris_example_species), 
      type = "III")
```

and note only petal length impacts sepal length. We could use *summary*
to see the impact

```{r}
summary(lm( Sepal_no_impact_species~ Petal_Length + Species, iris_example_species))
```

#### Impact of species but not petal length

We could also see an impact of species on sepal length, but no
relationship between petal length and sepal length.

```{r, echo=F}
set.seed(79)
iris_example_species$Sepal_no_relationship_petal <-rnorm(75) +
  c(rep(2,25), rep(4,25), rep(6,25))
```

```{r}
ggplot(iris_example_species, aes(x= Petal_Length, y = Sepal_no_relationship_petal, color = Species)) +
  geom_point()+
  ylab("Sepal Length") +
  xlab("Petal Length") +
  ggtitle("Impact of petal length and species on sepal length") +
    geom_smooth(method = "lm", se = F)

```

Analysis would indicate (assumption plots not shown here to allow focus
on interpreting interactions)

```{r}
Anova(lm( Sepal_no_relationship_petal~ Petal_Length * Species, iris_example_species), 
      type = "III")
```

no impact of interaction, so we could drop it

```{r}
Anova(lm( Sepal_no_relationship_petal~ Petal_Length + Species, iris_example_species), 
      type = "III")
```

and we find only species impacts sepal length. In that case, we need a
post-hoc follow up.

```{r}
summary(glht(lm( Sepal_no_relationship_petal~ Petal_Length + Species, iris_example_species), 
             linfct =mcp(Species = "Tukey")))
```

#### Impact of species and petal length, but no interaction

We also might see a difference among the species on sepal length *and* a
relationship between petal length and sepal length, but find the
relationship is the same for all species

```{r, echo=F}
set.seed(29)
iris_example_species$Sepal_no_interaction <- 
  iris_example_species$Petal_Length * 2 + rnorm(75) +
  c(rep(2,25), rep(2,25), rep(6,25))
```

```{r}
ggplot(iris_example_species, aes(x= Petal_Length, y = Sepal_no_interaction, color = Species)) +
  geom_point()+
  ylab("Sepal Length") +
  xlab("Petal Length") +
  ggtitle("Impact of petal length and species on sepal length") +
    geom_smooth(method = "lm", se = F)

```

Analysis would indicate (assumption plots not shown here to allow focus
on interpreting interactions)

```{r}
Anova(lm( Sepal_no_interaction~ Petal_Length * Species, iris_example_species), 
      type = "III")
```

no impact of interaction, so we could drop it

```{r}
Anova(lm( Sepal_no_interaction~ Petal_Length + Species, iris_example_species), 
      type = "III")
```

Given this, we could focus post-hoc tests on which species are different
than which others (since the relationship between sepal and petal length
is the same)

and we find only species impacts sepal length. In that case, we need a
post-hoc follow up.

```{r}
summary(glht(lm( Sepal_no_interaction~ Petal_Length + Species, iris_example_species), 
             linfct =mcp(Species = "Tukey")))
```

Here we would note that only *I. york* is different than the other
species.

#### Impact of species and petal length that differs among species

Finally, we could not there is an relationship between petal and sepal
length that differs among the species.

```{r, echo=F}
set.seed(31)
iris_example_species$Sepal_interaction <- 
  iris_example_species$Petal_Length * c(rep(-2, 25),rep(0,25), rep(5,25)) + 
  c(rep(2,25), rep(3,25), rep(4,25)) + rnorm(75)
```

```{r}
ggplot(iris_example_species, aes(x= Petal_Length, y = Sepal_interaction, color = Species)) +
  geom_point()+
  ylab("Sepal Length") +
  xlab("Petal Length") +
  ggtitle("Impact of petal length and species on sepal length") +
    geom_smooth(method = "lm", se = F)

```

Analysis would indicate (assumption plots not shown here to allow focus
on interpreting interactions)

```{r}
Anova(lm( Sepal_interaction~ Petal_Length * Species, iris_example_species), 
      type = "III")
```

interactions do exist. This means we can't interpret the "general"
relationship, so we need to look for each species using regression.

```{r}
summary(lm(Sepal_interaction ~ Petal_Length, 
         iris_example_species[iris_example_species$Species == "baruch",]))
Anova(lm(Sepal_interaction ~ Petal_Length, 
         iris_example_species[iris_example_species$Species == "baruch",]), 
      type="III")
summary(lm(Sepal_interaction ~ Petal_Length, 
           iris_example_species[iris_example_species$Species == "hunter",]))
Anova(lm(Sepal_interaction ~ Petal_Length, 
         iris_example_species[iris_example_species$Species == "hunter",]), 
      type="III")
summary(lm(Sepal_interaction ~ Petal_Length, 
           iris_example_species[iris_example_species$Species == "york",]))
Anova(lm(Sepal_interaction ~ Petal_Length, 
         iris_example_species[iris_example_species$Species == "york",]), 
      type="III")
```

Here we see that there is a significant negative relationship (F\~1,23 =
50.36, p\<0.001) between sepal and petal length for *I. baruch*, a
significant positive relationship (F\~1,23 = 269.53, p\<0.001) between
sepal and petal length for *I. york*,and no relationship (F\~1,23 =
1.63, p\<-0.21) between sepal and petal length for *I. hunter*.

## Extensions to multiple regression

Just as we can extend the 2-way ANOVA ideas to ANCOVA, it turns out we
can extend these ideas further to include even more variables (and their
interactions, if we want) using our linear model framework. At each
stage we are continuing to partition variance among factors and ask
(using our type III residuals) how "much" better a given factor makes a
model.

For example, we can return to our FEV data from the previous chapters
practice problems. Remember, we investigated the impact of age,

```{r}
#fev data####
fev <- read.table("http://www.statsci.org/data/general/fev.txt", header = T,
                  stringsAsFactors = T)
head(fev)
fev_age <- lm(FEV ~ Age, fev)
plot(fev_age)
Anova(fev_age, type = "III")
summary(fev_age)

#age plot####
ggplot(fev, aes(x=Age, y=FEV)) +
  geom_point(size = 3) +
  geom_smooth(method = "lm") +
  labs(y="FEV (liters)",
       x= "Age (years)",
       title ="FEV increases with age")
```

height,

```{r}
fev_height <- lm(FEV ~ Height, fev)
plot(fev_height)
Anova(fev_height, type = "III")
summary(fev_height)

#height plot####
ggplot(fev, aes(x=Height, y=FEV)) +
  geom_point() +
  geom_smooth(method = "lm") +
    labs(y="FEV (liters)",
       x= "Height (inches)",
       title ="FEV increases with height")
```

and gender

```{r}
fev_gender <- lm(FEV ~ Sex, fev)
plot(fev_gender) #anova is fine
Anova(fev_gender, type = "III")
summary(fev_gender)

#gender plot ####

#bar chart with error bars ####
library(Rmisc)
function_output <- summarySE(fev, measurevar="FEV", groupvars =
                               c("Sex"))

ggplot(function_output, aes(x=Sex, y=FEV)) +
  geom_col() +
  ylab("FEV") +
  geom_errorbar(aes(ymin=FEV-ci, ymax=FEV+ci), size=1.5) 
```

on outcomes and found all were significant predictors. However, these
variables are correlated. For example, there is a relationship between
height and gender

```{r}
height_gender <- lm(Height ~ Sex, fev)
plot(height_gender) #anova is fine
Anova(height_gender, type = "III")
summary(height_gender)

#gender plot ####

#bar chart with error bars ####
function_output <- summarySE(fev, measurevar="Height", groupvars =
                               c("Sex"))

ggplot(function_output, aes(x=Sex, y=Height)) +
  geom_col() +
  ylab("Height") +
  geom_errorbar(aes(ymin=Height-ci, ymax=Height+ci)) 
```

So we may want to know if we can do better with a larger model.

To begin with, notice we are slightly changing the question. We are
moving from hypothesis-based analysis to a decision to find the "best"
model. However, we are still trying to use a linear model framework, so
our predictor variables need to be independent (or at least somewhat
independent). We can consider this by noting the relationship among all
predictor variables. The *pairs* function is one way to do this; the
added portion below shows *r* values (correlations) among variables and
marks significant relationships using asterisks (any asterisk indicates
a p value of \< .05):

```{r}
panel.cor <- function(x, y, digits=2, prefix="", cex.cor) 
{
  usr <- par("usr"); on.exit(par(usr)) 
  par(usr = c(0, 1, 0, 1)) 
  r <- abs(cor(x, y)) 
  txt <- format(c(r, 0.123456789), digits=digits)[1] 
  txt <- paste(prefix, txt, sep="") 
  if(missing(cex.cor)) cex <- 0.8/strwidth(txt) 
  
  test <- cor.test(x,y) 
  # borrowed from printCoefmat
  Signif <- symnum(test$p.value, corr = FALSE, na = FALSE, 
                   cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                   symbols = c("***", "**", "*", ".", " ")) 
  
  text(0.5, 0.5, txt, cex = cex * r) 
  text(.8, .8, Signif, cex=cex, col=2) 
}
pairs(fev, lower.panel=panel.smooth, upper.panel=panel.cor)
```

Note we significant relationships among FEV and several variagles (e.g.,
age), but also among several predictor variables (e.gl, height and age).
This makes sense (people tend to grow for first 20ish years!), but the
r^2^ between height and age is only 64%.

```{r}
cor.test(~ Age + Height, fev)
```

While there is no hard limit, if 2 variables share an r^2^ value of
greater than 80%, only one should be included in the model. 60-70% is
even better.

For now, let's retain all factors and consider how we can carry out
model selection.

### Option 1: Run full model and interpet outcomes

One option is to construct a large model,

```{r}
fev_full <- lm(FEV ~ Age * Height * Sex * Smoker, fev)
```

check it,

```{r}
plot(fev_full)
```

and interpret outcomes if assumptions are met.

```{r}
Anova(fev_full, type="III")
```

This (and the following) approaches assume you chosen legitimate
predictor variables (you have a reason/mechanism for explaining their
impact).

### Option 2: Remove or add variables using p-values

Another option is to start with the full model and remove factors (or
their interactions) until all are "significant". You can do this by
building models manually or using automated approaches like the *drop1*
function.

```{r}
drop1(fev_full, test="F")
```

Notice it considers highest order interactions first. This indicates we
should drop the 4-way interaction term, so we do that and continue

```{r}
fev_full_working <- update(fev_full, .~.- Age:Height:Sex:Smoker)
drop1(fev_full_working, test = "F")
```

Now we can drop another interaction term

```{r}
fev_full_working <- update(fev_full_working, .~.- Age:Sex:Smoker)
drop1(fev_full_working, test = "F")
```

and another

```{r}
fev_full_working <- update(fev_full_working, .~.- Age:Height:Smoker)
drop1(fev_full_working, test = "F")
```

and so on until all interactions or main effects are significant (like
we see now). This approach requires nested models (you compare a model
without a given factor to one that has it - type III residuals!).

Alternatively, you can build a simple model

```{r}
fev_under <- lm(FEV ~ 1, fev)
```

and add factors to it until none are significant (not fully shown here).

```{r}
add1(fev_under, ~ Age + Height + Sex, test = "F")
fev_under_a <- update(fev_under, .~. + Age)
add1(fev_under_a, ~ . + Height + Sex, test = "F")
```

### Option 3: Compare models (nested or not) using AIC

You may have noted the AIC values above. AIC is "an information
criteria" that uses maximum likelihood..

<details>

<summary>Remember that? from [the chapter on comparing
proportions](Compare_proportions_among_populations.qmd){target="_blank"}</summary>

For an easy example of likelihood, let's go back to a one-sample example
of comparing a proportion to a sample and focus on finch data. We have 9
old and 9 young birds, so we have a signal of .5 for *p*. We can use
likelihood to calculate how likely our data was under multiple values of
p (ranging from 0 - 1, the only options here) and compare the
*likelihood* of those outcomes @white.

```{r, echo=F}
#what we observed in a vector
sequence <- c(rep(0,9), rep(1,9))
#
#write a likelihood function to solve for likelihood
likelihood <- function(sequence, p.parameter)
{
  likelihood <- 1
  
  for (i in 1:length(sequence))
  {
    if (sequence[i] == 1)
    {
      likelihood <- likelihood * p.parameter
    }
    else
    {
      likelihood <- likelihood * (1 - p.parameter)
    }
  }
  
  return(likelihood)
}

#then solve and plot
likelihood_df <- data.frame(possible.p = seq(0, 1, by = 0.001))
likelihood_df$likelihood <- likelihood(sequence, likelihood_df$p)
ggplot(likelihood_df, aes(x=possible.p, y=likelihood))+
  geom_line()+
  labs(title ='Likelihood as a Function of P when p = .5', 
       x = 'P',
       y = 'Likelihood')

```

Similar to calculating sum square errors from models, what is most
likely is what we saw, but we know there is always noise in the data.
Thankfully, it turns out the ratio of likelihood values follow a
$\chi^2$ distribution and can thus provide a p-value to compare possible
models. We will return to likelihood-based approaches later, in fact, as
they can be used for any dataset we can generate a model for and can be
used to compare multiple models.

</details>

to compare models. To do so, it obtains a likelihood value for a model
and takes a log of it. It then multiples that by -2 and adds a penalty
parameter multiplied by the number of parameters in the model. Why?
Because the algorithms used mean an extra parameter will always make a
model better (or at least no worse).

The true *AIC* process uses a penalty parameter of 2; other approaches
vary this. The Bayesian information criterion (BIC), for example, uses a
penalty parameter of of log(n), where n is the number of observations.
Given this formula, the "best" model has the **lowest** AIC value.

AIC and other information criteria can be used to compare nested models
(the value of adding or removing a single variable). This can be done in
a few ways. The default for *drop1* focuses on AIC

```{r}
drop1(fev_full)
```

but we can also automate this process using the *stepAIC* function

```{r}
stepAIC(fev_full)
```

```{r, echo=F, eval=FALSE}
#needs to be added
stepAIC(fev_under,fev_full, direction ="forward")
```

AIC and IC can also be used to compare non-nested models. For example,
the *dredge* function in the **MuMin** package takes a large model and
copmares all outcomes.

```{r}
library(MuMIn)
options(na.action = "na.fail")
auto <- dredge(fev_full)
library(rmarkdown)
paged_table(auto)
```

The output shows us which factors lead to the smallest AIC value (here
using the small sample correction, the AICc value). One benefit of this
approach is we may find multiple models have similar AIC values. Delta
(difference) values of \<2 mean models are supported [@modelse2004].

> Models having $\Delta_i$ ≤ 2 have substantial support (evidence),
> those in which 4 ≤ $\Delta_i$ ≤ 7 have considerably less support, and
> models having $\Delta_i$ \> 10 have essentially no support.

We can actually average supported models using their weight to find an
"final" model (code shown here, but note we only have one model meeting
this criteria and thus receive an error.

```{r, error=T}
model.avg(auto, subset = delta < 2)
```

While AIC and other IC may be an interesting approach, a few points
should be made.

-   AIC values are relative only to models fit with the same data.

    -   So there is no "good" AIC value

-   AIC outcomes actually correspond to using an $\alpha$ level of .15
    [@steyerberg2000], so AIC approaches typically favor larger models

-   AIC relies on large-sample approximations

    -   AICc corrects for small samples and can always be used.

### Option 4: Compare r^2^ (or adjusted r^2^) values

This option is noted here as it was more common in the past, but now it
should not be used. If it is used, however, one should compare adjusted
r^2^ values since the r^2^ values my be connected to larger but less
useful models.

## Final model checks

Regardless of the the method that is used, the linear models still need
to meet basic assumptions. For this reason, the final model (and
probably all those developed in route to getting there, though this
rarely happens) should be assessed for assumptions.

A new approach we have not used previously focuses on variance inflation
factors (vif) to ensure collinearity (more on this in next section)
isn't an issue. In general you want these values to be less than 5. When
working with normal linear models, especially those have factors that
have more than 2 levels, focusing on the generalized form of the vif for
predictors is appropriate.

```{r}
vif(fev_full_working, type="predictor")
```

## Important differences

One major difference between ANCOVA/ANOVA and multiple regression is
that multiple regression, especially with larger models, is often more
focused on outcomes/prediction than explanation. This differs from the
focus on null hypothesis significance testing (NHST) that we have been
focused on in previous chapters, but the mathematical approaches remain
the same via the use of the linear model.

## Next steps

These methods can be extended to other models that are used when linear
model assumptions are not met, which is the focus of the next chapter.
