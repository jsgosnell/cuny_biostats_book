---
title: Compare means among groups - update by Melaniea and Aimar
subtitle: More multiple populations!
bibliography: ../references.bib
editor: 
  markdown: 
    wrap: 72
---

<!-- COMMENT NOT SHOW IN ANY OUTPUT: Code chunk below sets overall defaults for .qmd file; these inlcude showing output by default and looking for files relative to .Rpoj file, not .qmd file, which makes putting filesin different folders easier  -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
source("../globals.R")
```

In the last chapter we introduced the idea of comparing parameters among
populations. Now we will extend those ideas to instances when continuous
data is collected.

## Example: Back to the iris

When we introduced NHST for continuous data, we focused on sepal length
from *I. virginica*.

```{r}
set.seed(42)
library(ggplot2)
ggplot(iris[iris$Species == "virginica",],
              aes(x=Sepal.Length)) +
  geom_histogram( fill="blue", color="black") +
  labs(title=expression(paste("Sepal lengths of ",italic("I. virginica"))),
       x= "Sepal length (cm)",
       y= "Frequency")
```

and tested if it was equal to a given value (7.0 cm)

$$
\begin{split}
H_O: \mu_{sepal \ length} = 7 \ cm \\ 
H_A: \mu_{sepal \ length} \neq 7 \ cm 
\end{split}
$$

We then considered how to assess these types of hypotheses using z, t,
Wilcoxon, and sign tests.

However, as we noted in the last chapter, we often instead have data
from multiple populations. For example, we may have data from 3 species
of flowers. We commonly see this data plotted as a bar chart with error
bars

```{r, echo = F, warning=F}
library(Rmisc)
function_output <- summarySE(iris, measurevar="Sepal.Length", groupvars =
                               c("Species"))

ggplot(function_output, aes(y=Sepal.Length, x=Species, fill=Species)) +
  geom_col(aes(fill=Species)) +
    geom_errorbar(aes(ymin=Sepal.Length-ci, ymax=Sepal.Length+ci)) +
  labs(title=expression(paste("Sepal lengths of ",italic("I. species"))),
       y= "Sepal length (cm)",
       x= "Species")
```

What do we test now and how?

<details>

<summary>More on wide vs long</summary>

Notice the iris dataset is in long format (each row is one flower).

```{r}
library(rmarkdown)
paged_table(iris)
```

It contains multiple pieces of information about that flower (e.g.,
sepal length, petal length) but only from that one flower. This is why
long format is more useful - when we start using multiple variables wide
format does not work very well.

</details>

## Welcome to ANOVAs

As previously noted, we can't compare heights among group. Height is a
random variable, and it's highly unlikely they will be exactly the same.
Seeing the actual data may help us remember this.

```{r}
ggplot(iris, aes(y=Sepal.Length, x=Species, color=Species)) +
  geom_jitter() +
  labs(title=expression(paste("Sepal lengths of ",italic("I. species"))),
       y= "Sepal length (cm)",
       x= "Species")
```

So we typically focus on a parameter that describes the distribution of
a focal trait, such as the mean. Just like binomial data, our hypotheses
can then be written several ways:

\\\\$$
\begin{split}
H_O: \mu_{sepal \ length, \ setosa} = \mu_{sepal \ length, \ virginica} = \mu_{sepal \ length, \ versicolor},\\
\textrm{there is no difference among the population means}\\
\textrm{all means are the same}\\
H_A: \textrm{all means are not equal to each other;}\\
\textrm{at least one mean is different than the others}\\
\textrm{(a little odd to write mathematically!)}\\ 
\end{split}
$$

Given that, our overall idea is to consider if the data are better
explained by an overall group average or by species-specific averages.
To visualize this, we could use this image.

```{r}
library(viridis)
ggplot(iris, aes(Species,Sepal.Length)) + 
  geom_jitter(size = 3) +
  geom_errorbar(aes(ymin=Sepal.Length, ymax=Sepal.Length, color="group means"), size=1.5, 
                data = function_output) +
    geom_hline(aes(yintercept=mean(Sepal.Length),  color = "overall average"),  size=1.5)+
  scale_color_manual(values = c("#619CFF","#F8766D"))+
    labs(title=expression(paste("Sepal lengths of ",italic("I. species"))),
       y= "Sepal length (cm)",
       x= "Species", 
       color= "Focus")
```

Like our earlier considerations of variance and SSE, the data will
obviously be fit better by species-specific averages. Its impossible for
them to do worse than the overall average, and at worst they all *are*
the group average. However, we should also remember that these are
samples, so we know sampling error is an issue. Therefore, we have to
consider if the species-specific averages do enough of a better job
explaining the data to warrant using them. To put this in our SSE and
hypothesis framework, we need to consider if a more complicated view of
the world is worth it.

To test this, we can (as always) carry out a sampling experiment. The
general idea is that species does not matter (just like we saw in
contingency analysis). Given that, we can draw samples that match our
respective sample sizes for each population from a single population.
The mean can be set as the pooled mean for the data (since under the
null tested factors don't matter). Since we don't know or set sigma, we
can again estimate it from the data. One such sample might look like

```{r}
set.seed(7)
variance_estimate <- sum((function_output$N -1) * (function_output$sd)^2)/(sum(function_output$N)-length(function_output$N))
mean_sepal <- mean(iris$Sepal.Length)
simulated_data <- data.frame(Species=c(rep("setosa", 50), 
                                       rep("versicolor", 50),
                                       rep("virginica",50)),
                             Sepal.Length=rnorm(150, mean_sepal, 
                                          sd= sqrt(variance_estimate)))

function_output <- summarySE(simulated_data, measurevar="Sepal.Length", groupvars =
                               c("Species"))

ggplot(simulated_data, aes(Species,Sepal.Length)) + 
  geom_jitter(size = 3) +
  geom_errorbar(aes(ymin=Sepal.Length, ymax=Sepal.Length, color="group means"), size=1.5,
                data = function_output) +
    geom_hline(aes(yintercept=mean(Sepal.Length),  color = "overall average"),  size=1.5)+
  scale_color_manual(values = c("#619CFF","#F8766D"))+
    labs(title=expression(paste("Sepal lengths of ",italic("I. species"))),
       y= "Sepal length (cm)",
       x= "Species", 
       color= "Focus")
```

As expected, under the null hypothesis the overall and species-specific
averages are closer. Now that we have the data, however, we are stuck
with a new question: What is our test statistic?

In general, using a difference among means makes sense for cases when we
have only 2 populations. This will be used when we introduce t-tests.
However, it does not work for 3+ populations, so we will need a
different approach.

ANOVAs offer an approach that can be used for any group of 2+
populations when certain assumptions are met. ANOVAs stands for
*analysis of variance* which may seem odd given our hypotheses are
focused on means. However, the idea (not fully developed here) is that
we can get an overall estimate of variance by

-   calculating variance for each point around its respective
    group-specific mean

    -   this leads to 3 estimates of variance, which we can multiply by
        (n~j~-1) to account for differences in sample size and then
        divide by the number of samples to get an overall estimate of
        variance
        -   this is referred to as mean square error

    $$
    \begin{split}
    \textrm{Remember, }s^2 = \frac{\sum_{i=1}^{n} (Y_{i}-\overline{Y})^2}{n-1} \sim \sigma^2\\
    \textrm{So if we have j groups, for each group we can see}\\
     s_j^2 = \frac{\sum_{i=1}^{n_j} (Y_{ij}-\overline{Y_j})^2}{n_j-1} \sim \sigma_j^2\\
     \textrm{which we can combine to estimate }\sigma^2_{overall}\\
     \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 ...(n_j-1)s_j^2}{n_1+n_2+...n_j-j} = s_{overall}^2\\
    \end{split}
    $$

-   calculating variance for each group mean around the overall mean

    -   this can be multiplied by $n$ to get $\sigma^2$

    -   this is referred to as mean square treatment

$$
        \begin{split}
        \frac{\sum_{j=1}^{j} (\overline{Y_j}-\overline{\overline{Y}})^2}{j-1} = \frac{s^2}{n} \\
        \textrm{where j is the number of groups and }\ \overline{\overline{Y}}\ \textrm{is the overall mean.}\\
        \end{split}
$$

In other words, variance among groups should be equal to variance within
groups. Under the null hypothesis, the ratio of these estimates should
tend towards 1. We can see this using a sampling experiment.

```{r}
#sample
  ratio <- data.frame(rep = 1:10000, mse = rep(NA,10000), 
                      msg = rep(NA,10000), ratio = rep(NA,10000))
for(i in 1:10000){
    setosa <- rnorm(50, mean_sepal, sd= sqrt(variance_estimate))
    versicolor <- rnorm(50, mean_sepal, sd= sqrt(variance_estimate))
    virginica <- rnorm(50, mean_sepal, sd= sqrt(variance_estimate))
    mean_overall <- mean(c(setosa, versicolor, virginica))
    ratio$mse[i] <- (49 * var(setosa) + 49 * var(versicolor) + 49 * var(virginica))/(150 - 3)
    ratio$msg[i] <- (50 * (mean(setosa)-mean_overall)^2 + 
                 50 * (mean(versicolor)-mean_overall)^2 + 
                 50 * (mean (virginica)-mean_overall)^2)/2
    ratio$ratio[i] <- ratio$msg[i]/ratio$mse[i]
}
  

ggplot(ratio, aes(ratio)) +
    geom_histogram(aes(y=..count../sum(..count..)), fill = "orange", bins=15)+
    labs(main = "Ratio under null hypothesis", y= "Probability")
```

Using this approach, we could determine how unusual our data were (get a
p-value!). In our example, we found a signal of
`r round(summary(lm(Sepal.Length~Species, iris))$fstatistic[1],3)`.
Looking at the x-axis above, it's obvious getting a sample this or more
extreme is going to be very rare by chance!

However, finding a distribution that approximates this shape would make
future work easier. It turns out all the squared terms above lead to
this being a rato of $\chi^2$ distributions, where the numerator has
degrees of freedom j-1 (# of groups - 1) and the denominator has degrees
of freedom n-j-1 (the "lost" degrees of freedom are used to estimate
group and overall means). We call this the F distribution.

```{r}
ggplot(ratio, aes(ratio)) +
    geom_histogram(aes(y=..count../sum(..count..)), fill = "orange", bins = 12) +
    labs(main = "Ratio under null hypothesis", y= "Probability")+
    stat_function(fun = df, args = list(df1 =2, df2 = 147), color = "green")   

```

This allows us to estimate the p-value using distributional assumptions.

<details>

<summary>Why do we sometimes talk about sums of squares?</summary>

You should also note this means we can partition the variance for any
given observation as its distance from its group mean and its group
means distance from the overall. It turns out when we do this, we can
*decompose* the total sums of squares (similar to variance but not
divided!) into those attributed to the distance of points from the group
mean and the distance of group means from the grand mean (not proven
here). These are additive (equal to) to the total sums of squares. .
[For a proof (and more links!)
see](https://raw.githubusercontent.com/StatProofBook/StatProofBook.github.io/master/P/anova1-pss.md){target="_blank"}
@soch2024; main part reproduced below:

**Theorem:** Given one-way analysis of variance,

$$ 
y_{ij} = \mu_i + \varepsilon_{ij}, \; \varepsilon_{ij} \overset{\mathrm{i.i.d.}}{\sim} \mathcal{N}(0, \sigma^2)
$$

sums of squares can be partitioned as follows

$$
\mathrm{SS}_\mathrm{tot} = \mathrm{SS}_\mathrm{treat} + \mathrm{SS}_\mathrm{res}
$$

where $\mathrm{SS} _\mathrm{tot}$ is the \*\*total sum of
squares*\](/D/tss)**,*** $\mathrm{SS} _\mathrm{treat}$ is the treatment
sum of squares **(sometimes call *explained* sum of squares, and**
$\mathrm{SS} _\mathrm{res}$ is the residual sum of squares (sometimes
called *sum squares error*).

**Proof:** The total sum of squares for one-way ANOVA is given by

$$ 
\mathrm{SS}_\mathrm{tot} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y})^2
$$

where $\bar{y}$ is the mean across all values $y_{ij}$. This can be
rewritten as

$$ 
\begin{split}
\sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y})^2 &= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left[ (y_{ij} - \bar{y}_i) + (\bar{y}_i - \bar{y}) \right]^2 \\
&= \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left[ (y_{ij} - \bar{y}_i)^2 + (\bar{y}_i - \bar{y})^2 + 2 (y_{ij} - \bar{y}_i) (\bar{y}_i - \bar{y}) \right] \\
&= \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 + \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y})^2 + 2 \sum_{i=1}^{k} (\bar{y}_i - \bar{y}) \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i) \; .
\end{split}
$$

Note that the following sum is zero

$$ 
\sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i) = \sum_{j=1}^{n_i} y_{ij} - n_i \cdot \bar{y}_i = \sum_{j=1}^{n_i} y_{ij} - n_i \cdot \frac{1}{n_i} \sum_{j=1}^{n_i} y_{ij} \; ,
$$

so that the sum in in the original equation reduces to

$$ 
\sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y})^2 = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y})^2 + \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 \; .
$$

With the treatment sum of square for one-way ANOVA

$$ 
\mathrm{SS}_\mathrm{treat} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (\bar{y}_i - \bar{y})^2
$$

and the residual sum of squares for one-way ANOVA

$$ 
\mathrm{SS}_\mathrm{res} = \sum_{i=1}^{k} \sum_{j=1}^{n_i} (y_{ij} - \bar{y}_i)^2 \; ,
$$

we finally have:

$$ 
  \mathrm{SS}_\mathrm{tot} = \mathrm{SS}_\mathrm{treat} + \mathrm{SS}_\mathrm{res} \; 
$$

This fact, and some clever math tricks, made them easy to work with.
Eventually, however, we are focusing on the fact we are estimating the
variance in multiple ways.

</details>

<details>

<summary>and what is $\chi^2$</summary>

Remember from the notes on contingency analysis that a $\chi^2$
distribution is a Z variate squared.It's defined by degrees of freedom
equal to the number of free variates it is summed over.

</details>

## Welcome to the linear model

An ANOVA is just one case of a linear model. We will fully explore these
later, but noting this now is useful in that all linear models have the
same sets of assumptions. In general, linear models assume the
*residuals* of the model are are independent, identically distributed,
and follow a normal distribution. You'll sometime see this written as

$$
\epsilon \approx i.i.d.\ N(\mu,\sigma)
$$

But what does this mean?

Residuals are the distance between a measurement and its model-predicted
value. A closely related term, error, is actually the distance between a
measurement and the unknown population mean for a group. Linear models
assume the residuals are independent of each other (this follows from
independent data points) and that their spread (around their predicted
values) is normally distributed and similar for all points.

Understanding this explains two key points. The residuals, not the data,
need to be normally distributed. Also, we have to build the model to get
the residuals, then we check the assumptions.

We can do this in R using the *lm* function. This approach also lets us
use a single set of functions to build many model types. As always,
there are many ways to do anything in R, so there are specific ANOVA
functions that we will not introduce here.

For our data, we can build an lm object

```{r}
iris_anova <- lm(Sepal.Length~Species, iris)
```

then use *plot* to check the assumptions.

```{r}
plot(iris_anova)
```

These 4 plots focus on the residuals (not the data).

-   The **Residuals vs fitted** plot allows us to see if the residuals
    are identically distributed - we want to see a flat red line and no
    structure to the residuals in regards to their spread or location.
    Note we only have 3 fitted values here (matching our 3 group means),
    so will see "lines" of data in one-way ANOVAs (another name for what
    we are doing here).\
-   The **Q-Q Residuals** plot allows us to assess normality - points
    should be on the line
-   The other 2 graphs give show different forms of residuals against
    the fitted values. We will return to them later.

If our assumptions are met, we can look at the output. One way to do
this is using the *summary* function.

```{r}
summary(iris_anova)
```

This output may be confusing, however. The overall p value shown in the
bottom right is for the entire model - that works for now, but soon
won't. We also see individual p values for 2 levels of species, plus an
odd intercept term.

These are model artifacts and may be confusing. By default (based on
contrasts method), R lets the first factor level (typically
alphabetical) be an intercept for the linear model, and then considers
the other factor levels as deviations from that. It also shows if all
the resulting *Estimates* are significantly different from 0. Note this
means a significant intercept term does not mean your groups actually
differ. Other approaches to these will be noted later.

Given these issues, why use the summary command? It does present some
other useful information. For example, the R^2^ values is a measure of
how much variation the model explains. It can range from 0 (the model
explains nothing) to 1 (all residuals are zero, in this case meaning all
members of a given species have the exact same height). The
adjusted-R^2^ value is similar, but it adjusts the measure to account
for the fact more complex models will always explain more variation.

You can also remove the intercept to get group estimates for all groups

```{r}
summary(lm(Sepal.Length~Species-1, iris))
```

Note these match our group means, which is good, but the overall p value
is now less useful (it compares our data to a null that assumes
everything is equal to 0), and the output is still confusing.

Since we are doing an omnibus test, what we typically want is a single p
value associated with our factor of interest (species in this case). To
get that, we'll use the *Anova* function from the **car** package.

```{r}
library(car)
Anova(iris_anova, type="III")
```

<details>

<summary>What does type="III" mean</summary>

Here, the *type* argument is noting how residuals, and thus sums of
squares, should be calculated. Residuals can be calculated in multiple
ways. For simple models (those with one variable, which we have now) and
those with balanced designs (the same number of observations for each
grouping, coming back to this next chapter), these approaches will lead
to the same answer. However, we'll introduce these ideas here and return
to them later.

Let's show that here and briefly look at the approaches.

-   Type I sums of squares calculate residuals (and thus sums of
    squares) based on the order in which factors are added to a model.
    This is why they are also known as sequential sums of squares. Since
    we only have one factor now, this distinction isn't important, but
    we'll come back to it. The **Anova** function won't calculate these,
    but we can use another approach (don't use this otherwise!)

    ```{r}
    summary(aov(iris_anova))
    ```

-   Type 2 sums of squares do not consider interactions (which we
    haven't either yet, so again, we'll come back to it).

    ```{r}
    Anova(iris_anova, type="II")
    ```

So for our simple design, these all look the same. However, if we have
an unbalanced design, things change a little:

```{r}
#from https://jennybc.github.io/purrr-tutorial/ls12_different-sized-samples.html
library(purrr)
library(tidyr)
set.seed(19)
iris_unbalanced <- iris %>%
  group_by(Species) %>% 
  nest() %>%            
  ungroup() %>% 
  mutate(n = c(50, 30, 20)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  dplyr::select(-data) %>%
  unnest(samp)

iris_unbalanced_anova <- lm(Sepal.Length~Species, iris_unbalanced)
```

Now's lets compare our different sums of squares.

```{r}
summary(aov(iris_unbalanced_anova))
```

```{r}
Anova(iris_unbalanced_anova, type="II")
```

```{r}
Anova(iris_unbalanced_anova, type="III")
```

Wait..they are the same. Let's try even more unbalanced data

```{r}
set.seed(19)
iris_really_unbalanced <- iris %>%
  group_by(Species) %>% 
  nest() %>%            
  ungroup() %>% 
  mutate(n = c(50, 4, 2)) %>% 
  mutate(samp = map2(data, n, sample_n)) %>% 
  dplyr::select(-data) %>%
  unnest(samp)

iris_really_unbalanced_anova <- lm(Sepal.Length~Species, iris_really_unbalanced)
```

```{r}
summary(aov(iris_really_unbalanced_anova))
```

```{r}
Anova(iris_really_unbalanced_anova, type="II")
```

```{r}
Anova(iris_really_unbalanced_anova, type="III")
```

That's because R is assuming variance is the same for each of the
groups, which means unequal sample sizes isn't an issue. However, we
didn't test this assumption, and it would be really hard to given the
small sizes of some groups. In the next chapter, we'll discuss these
issues more. For now, we'll get in the habit of using *type="III*" for
this class.

</details>

Doing this we see Species has a significant impact on explaining
variation in the data (and our very high F value). So we reject the null
hypothesis that mean sepal length does not differ among species. Now
what?

## Post-hoc tests

Just like for a multi-population $\chi^2$ tests, we need to do follow-up
tests to compare groups while controlling for the FWER. For linear
models (and more), we will use the *glht* function from the **multcomp**
package to conduce these tests.

```{r}
library(multcomp)
compare_cont_tukey <- glht(iris_anova, linfct = mcp(Species = "Tukey"))
summary(compare_cont_tukey)
```

Our first approach uses a new (to us) method. It is **not exactly**
Tukey's method (very confusing) but is closely related to it. Tukey's
method is focuses on all possible pair-wise comparisons and controls for
the FWER using a studentized range approach (not fully developed here,
but similar to z-transform but focused on the range of means and using
the estimated standard deviation; similar to t-statistic in this aspect
and also developed by Student). It is also called Tukey's Honestly
Significant Difference/HSD, Tukey-Kramer method, and many other names.
In *glht*, specifying "Tukey" tells the program to do all possible pairs
comparison (like Tukey's method). The post-hoc control for FWER,
however, uses a slightly different approach that can handle interactions
(still to be explained) and some other things a little bit easier.

Using this approach we see that all species differ significantly from
all others; the output also provides estimates of the differences, which
match up with our model summary output.

We can also use the methods we previously discussed such as Bonferroni
and FDR. These requre us to set up the comparisons, which also means we
can limit our number of tests if so desired. For example, we could focus
only on differences with *I. virginica*.

```{r}
compare_virginica_only <- glht(iris_anova, linfct = mcp(Species = 
                                                                c("virginica - versicolor = 0", 
                                                                  "virginica - setosa = 0")))
```

After setting up the comparison, we can specify the method to use to
correct for FWER,

```{r}
summary(compare_virginica_only, test=adjusted("holm")) 
summary(compare_virginica_only, test=adjusted("fdr")) 
```

Note for our small number of tests and relatively large differences in
means and large sample sizes, differences in p values are minimal.

There are instances when FWER are not an issue and thus p values do not
need to be adjusted. This occurs when we explore *orthogonal contrasts*.

··

## An extra example: what happens if we have more than one explanatory variable?

```{r}
cropdata <- read.csv("content/chapters/cropdata.csv", header = TRUE, colClasses = c("factor", "factor", "factor", "numeric"))
```

This data contains information on crop yields explained by the type of
fertilizer used on the fields and/or planting density. These are two
factors that could potentially affect the crop yield, so finding the
impact on each could bring some useful information.

We can first check how the data looks:

```{r}
summary(cropdata)
```

We could first a model first to visualize how fertilizer type affects
the yield, under the null hypothesis that:

$$
\begin{split}
H_O: \mu_{\textrm{yield of fertilizer 1}} = \mu_{\textrm{yield of fertilizer 2}}....\textrm{for all fertilizers}\\
H_A: \mu_{\textrm{yield of fertilizer 1}} \neq \mu_{\textrm{yield of fertilizer 2}}....\textrm{for all fertilizers}\\
\end{split}
$$

```{r}
model <- lm(yield ~ fertilizer, data = cropdata)
Anova(model, type = 'III')

```

We see that fertilizer type actually has an impact on the mean crop
yield. However, it could be that adding the extra factor added even more
information, this is considered in what is called a **two-way anova**:

```{r}
model <- lm(yield ~ fertilizer + density, data = cropdata)
Anova(model, type = 'III')
```

Also density seems to have a significant impact on the total crop yield!
Indeed, this second model seems to have improved the model: the residual
variance has decreased from 36 to 31.

This definitely adds more information if someone wanted to know how
fertilizer and planting density affected their crop yields. But what if
those two factors interact with each other, in a way that, i.e.,
fertilizer effect is limited (or enhaced) by the density of the plants?

We can check for that by considering their interaction:

```{r}
model <- lm(yield ~ fertilizer*density, data = cropdata)

Anova(model, type = 'III')
```

The interaction (shown by ":") does not seem to be significant, so its
mostly safe to assume fertilizer effect on crop yield is not conditioned
by plant density.

This already seems like a useful conclusion! But we still have to
remember to check for the linear model assumptions, or our conclusions
might be totally wrong!

```{r}
par(mfrow=c(2,2))
plot(model)
par(mfrow=c(1,1))
```

From the results we see that it is more or less normal (the results are
not going to be perfect in reality, so these results are pretty good.)

The Q-Q Residuals plot have a slope very similar to 1. And the other
plots depict apretty horizontal line. Now that we checked for
homoscedasticity we'll check for a post-hoc test. We know that according
to fertilizer and plant density, the mean yield is going to be
different. But we don't know more than that. We don't know which
fertilizer is driving the significance and how much plant density is
driving the significance.

So if we use Tukey's post-hoc to test for individual comparisons while
accounting for the family-wise-error-rate...

```{r}
model <- lm(yield ~ fertilizer, data = cropdata)
compare_cont_tukey <- glht(model, linfct = mcp(fertilizer = "Tukey"))
summary(compare_cont_tukey)

```

From the results we see that there is a significant difference between
fertilizers group 3-2 and 3-1. But no significant difference between
groups 2 and 1. Fertilizer group 3 has a 60% more yield that fertilizer
group 1 and fertilizer group 3 has a 42% more yield that group 2.

```{r}
model <- lm(yield ~ density, data = cropdata)
compare_cont_tukey <- glht(model, linfct = mcp(density = "Tukey"))
summary(compare_cont_tukey)
```

Regarding plant density, there is a significant difference between 2 -1.
From the diff variable, we see that group 2 has a 46% more yield than
group 1.

We can note that doing this post-hoc test is similar to doing a regular
t-test, as we are only comparing two levels in density:

```{r}
t.test(yield~density, cropdata)
```

Lasly, we could also plot the data and see if that brought more useful
clarity:

```{r}
# one box per variety
ggplot(cropdata, aes(x=fertilizer, y=yield, fill=density)) + 
    geom_boxplot() +
    facet_wrap(~fertilizer, scale="free_x")

```

## A little deeper into linear models

Let's return to linear models to help explain orthogonal contrasts (and
some other things). Linear models are a sysem of equations (a matrix),
where

$$
\begin{split}
Y=X\beta+\epsilon, \textrm{ where }\\
\textrm{Y is our observations (an nx1 matrix)}\\
\textrm{X is a matrix showing our explanatory variables (an nxk matrix)}\\
\beta \textrm{ is our coefficient matrix(an kx1 matrix)}\\
\epsilon \textrm{is a matrix (an nx1) of residuals)}\\
\end{split}
$$

In our case, $\beta$ is simply a 3x1 matrix where each entry is a
species average (or one is an intercept and other two are distances from
it - same thing) and X is a matrix with dummy variables (1 or 0)
indicating which group each observation belongs too. X is sometimes
called a model or design matrix. We can see this using R. First, we can
pull the design matrix from our model object

```{r}
paged_table(data.frame(model.matrix(iris_anova)))
```

We can also pull the model coefficients, which form our $\beta$ matrix,
and place them in the correct orientation.

```{r}
matrix(as.numeric(iris_anova$coefficients), ncol=1)
```

So for our first observation, which is

```{r}
iris[1,]
```

We would multiply (remember, rows get multiplied by columns in matrices)

```{r}
1*5.006 + 0*.930+0*1.58
```

This explains why all our fitted values are one of three values! We can
also see our observation minus prediction

```{r}
iris[1,"Sepal.Length"]-1*5.006 + 0*.930+0*1.58
```

matches our first model residual

```{r}
iris_anova$residuals[1]
```

Understanding this general setup explains a few things. When models get
more complicated you may see errors or warnings related to singularity.
This occurs when X^T^X isn't invertible(linear algebra!), which it needs
to be to find $\beta$. This occurs if columns in your design matrix are
not independent and are actually linear combinations of each other. This
happens when you have highly related measurements (we'll discuss
correlation eventually so you can actually measure this!). We will use
similar manipulations to eventually find the $\hat{H}$ matrix when we
consider Cook's Distance (in regression chapter). Degrees of freedom are
similarly related to the number of estimated coefficients the model
required (the number of rows in the $\beta$ matrix).

Returning to our contrasts, note when we do post-hoc tests we are
effectively testing for differences in $\beta$ values. We can put these
"tests" in a similar system of equations/matrix. For these tests, the
coefficients have to equal 0. For pair comparisons, that means we have 1
for one coeffcient and -1 for the other (for example, (1,-1,0). However,
we can also compare one coefficient to the average of the others
(2,-1,-1). We could write these two contrasts as

$$
\begin{bmatrix}
1 & -1 & 0 \\
2 & -1 & -1 \\
\end{bmatrix}
$$

A group of contrasts are orthogonal if the sum of the multiplied
coefficients from each column equals zero. In this case

```{r}
1*2 + -1*1 + 0*1
```

does not equal 0, so these are not orthogonal contrasts. This is also
because I can add add the first and third column and get the second
(columns are not independent). However, if we instead carried out these
contrasts

$$
\begin{bmatrix}
2 & -1 & -1 \\
0 & 1 & -1\\
\end{bmatrix}
$$

they would be independent. There are other options as well, as we can
always find a number of orthogonal contrasts equal to the number of
groups being compared minus one.

Resulting p values would not required correction for FWER. We can
specify contrasts like this using *glht*. Below I do the same matrix,
but note I set the maximum entry to 1 so that estimate of mean
differences aren't doubled.

```{r}
contr <- rbind("setosa - versicolor - virginica" = c(1, -.5,-.5),
               "versicolor - virginica" = c(0,1,-1))
summary(glht(iris_anova, linfct = mcp(Species = contr)), test=adjusted("none"))
```

## Displaying output of post-hoc tests

Output from post-hoc tests is often displayed using compact letter
display. Groups that are not significantly differently share the same
letter (so in this case they all have different letters).

```{r}
cld_output <- fortify(cld(compare_cont_tukey))
cld_output$Species <- cld_output$lhs

function_output <- summarySE(iris, measurevar="Sepal.Length", groupvars =
                               c("Species"))

function_output <- merge(function_output, cld_output)

ggplot(function_output, aes(y=Sepal.Length, x=Species, fill=Species)) +
  geom_col(aes(fill=Species)) +
    geom_errorbar(aes(ymin=Sepal.Length-ci, ymax=Sepal.Length+ci)) +
  labs(title=expression(paste("Sepal lengths of ",italic("I. species"))),
       y= "Sepal length (cm)",
       x= "Species")+
  geom_text(aes(label=letters,y=Sepal.Length+3*ci))
```

Other options include plotting the differences in means

```{r}
plot(compare_cont_tukey)
```

```{r, eval=F, echo=F}
#hidden for now as library not working
#Another option using the ggstatsplot package.  
library(ggstatsplot)
ggstatsplot::ggbetweenstats(
  data = PlantGrowth,
  x = group,
  y = weight,
  pairwise.comparisons = TRUE,
  pairwise.display = "all",
  p.adjust.method = "Tukey"
)
```

### emmeans: another option

Another popular package for conducting posthoc comparison in R is
emmeans. The package has a great starters guide
[here](https://cran.r-project.org/web/packages/emmeans/vignettes/AQuickStart.html){target="_blank"}

## T-test connections

So far we have focused on comparing means among multiple groups. This
can include include comparing means between only 2 groups (which we
already do for the post-hoc tests). In doing this we also introduced new
post-hoc tests and the ideas of a linear model.

The linear model framework will unify most of the remaining tests we
learn in class. In fact, several tests we've already learned can be
formulated this way. This is extremely useful given we want statistics
to be a related set of tests in a comprehensive framework.

There are many ways to teach statistics, however, and a long history of
tests. Many textbooks and approaches build up from one sample tests by
moving to two-sample t-tests. These tests bridge the logic noted above
and the approach we used for single-sample t-test. This is because the
t-distribution is a special case of the F distribution. It occurs when
the square root of an F distribution with 1 degree of freedom in the
numerator is considered. Thus the degrees of freedom associated with a
t-test will be equal to the degrees of freedom associated with the
denominator of the associated F-test.

2-sample t-tests may also an easier approach to first considering
differences among groups since with only 2 populations the difference in
means may be considered. However, it can be shown (not here) this is
simply a rearrangement of our exploration of variances.

To demonstrate this, let's only consider two species. Note

```{r}
two_species_subset <- iris[iris$Species!="setosa",]
t.test(Sepal.Length ~ Species, two_species_subset, var.equal=T)
```

yields the same p-value as

```{r}
Anova(lm(Sepal.Length ~ Species, two_species_subset), type="III")
```

Also note the t statistic is the square root of the F statistic.

```{r}
-5.6292^2
```

Note the t.test function can also use columns holding data from each
population as arguments as opposed to the formula interface, but we will
not use that approach here.

The var.equal=T argument, however, is not the default in R, and this
assumption is one of the major differences in 2-sample t-tests and F
tests. Remember, ANOVAs and t-tests both require estimates for sigma. If
we do not assume the variances are equal for each group, then the best
way to estimate the variance is to calculate the variance for each group
and take a weighted (by sample size mean). This approach is known as the
Behren-Fisher or Welsh t-test.

```{r}
t.test(Sepal.Length ~ Species, two_species_subset)
```

The resulting statistics has a distrubtion that can be approximated by a
t-distribution, but the associated degrees of freedom can can be
non-integer (decimal) and less than (n~1~+ n~2~ - 2).

This means the basic assumptions for 2-sample t-tests are independent
data points, groups show the same variance, and means are normally
distributed. Much like the one-sample t-test, the central limit theorem
implies assumptions about the mean distribution are commonly met.
However, if they are not met, we have a few common options.

## Non-parametric connections

Options for when assumptions of the t- and F-tests (ANOVAs) are
presented below. Note given the history of t-tests being considered
apart from ANOVAs, some functions only work with less than 2 populations
while others work with three or more. However, the overall approaches
are similar.

### Ranks: Wilcoxon/Mann-Whitney U and Kruskal-Wallis test

We can extend the Wilcoxon test to 2-samples. To do so, we rank the data
points from smallest to largest. The ranks are then used to calculate a
U statistic. The statistic sums the ranks for each group (r), then uses
them to calculate

$$
U_1 = n_1n_2+\frac{n_1(n_1+1)}{2}-r_1
$$ The U statistics is calculated for each group. The larger U value is
then taken and used to compute a p value. We can calculate this using
the *wilcox.test* function,

```{r}
wilcox.test(Sepal.Length ~ Species, two_species_subset)
```

This test assumes the two distributions being considered have similar
shape (not that the resulting means are normally-distributed). If you
remove the default continuity correction (applied as we approximate
discrete data with a continuous distribution)

```{r}
wilcox.test(Sepal.Length ~ Species, two_species_subset, correct=F)
```

We get the same result as the Kruskal-Wallis test

```{r}
kruskal.test(Sepal.Length ~ Species, two_species_subset)
```

which is a rank-based test that can be applied to 3+ populations.

```{r}
kruskal.test(Sepal.Length ~ Species, data = iris)
```

If we have more than three populations and this omnibus test reveals a
significant p-value, we can follow it up with appropriate post-hoc
tests.

```{r}
pairwise.wilcox.test(iris$Sepal.Length, 
                          iris$Species, 
                          p.adjust.method="holm")
```

### Sign/Binary approach

For a single sample, we also considered the sign/binary test. We will
return to this test in the next chapter, as it does not work for data
from independent samples.

### Bootstrapping

Another option is to extend the bootstrapping option. Although we could
again develop a simulation using the *boot* function again, here we
again use the **MKinfer** package.

```{r}
library(MKinfer)
boot.t.test(Sepal.Length ~ Species, two_species_subset)
```

note we can also run this test without assuming variances are different.

```{r}
boot.t.test(Sepal.Length ~ Species, two_species_subset, var.equal=T)
```

Both these approaches also show the corresponding t-test results, but
note you should choose which test you plan to use before seeing the
results!

For more than 2 groups, the *t1waybt* function in the **WRS2** package
can allow comparison.

```{r}
library(WRS2)
t1waybt(Sepal.Length~Species, iris)
```

If needed, the *mcppb20* package allows for appropriate post-hoc
comparisons.

```{r}
bootstrap_post_hoc <- mcppb20(Sepal.Length~Species, iris)
bootstrap_post_hoc_df <-data.frame(bootstrap_post_hoc$comp)
bootstrap_post_hoc_df$adjusted_p <- p.adjust(as.numeric(bootstrap_post_hoc$comp[,6]), "holm")
bootstrap_post_hoc_df$Group <- factor(bootstrap_post_hoc_df$Group)
library(plyr)
bootstrap_post_hoc_df$Group <- revalue(bootstrap_post_hoc_df$Group,
                                       setNames(                                      bootstrap_post_hoc$fnames,as.character(1:length(bootstrap_post_hoc$fnames))))
bootstrap_post_hoc_df$Group.1 <- factor(bootstrap_post_hoc_df$Group.1)

bootstrap_post_hoc_df$Group.1 <- revalue(bootstrap_post_hoc_df$Group.1,
                                       setNames(                                      bootstrap_post_hoc$fnames,as.character(1:length(bootstrap_post_hoc$fnames))))
bootstrap_post_hoc_df
```

### Permutation

A new option when comparing groups (2 or more) is known as the
permutation test. We encountered a similar approach when we learned
about the Fisher's test for binomial data. Using this approach, we can
move the measurements between measured populations, calculate test
statistics, and consider how unusual our observed statistic was (a p
value!). We can do this for 2

```{r}
library(coin)
independence_test(Sepal.Length ~ Species, data =  two_species_subset)
```

or 3+ populations

```{r}
independence_test(Sepal.Length ~ Species, data =  iris)
```

Post-hoc test options are available in the **rcompanion** package.

```{r}
library(rcompanion)
pairwisePermutationTest(Sepal.Length ~ Species,
                             data = iris,
                             method="holm")
```

## Next steps

Our following chapters will extend ANOVAs to consider the impact of
multiple measured categories. In doing so, we will also explain paired
t-tests and sign tests for paired data.
