---
title: "Compare means among groups"
bibliography: ../references.bib
---

<!-- COMMENT NOT SHOW IN ANY OUTPUT: Code chunk below sets overall defaults for .qmd file; these inlcude showing output by default and looking for files relative to .Rpoj file, not .qmd file, which makes putting filesin different folders easier  -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

Remember you should

-   add code chunks by clicking the *Insert Chunk* button on the toolbar
    or by pressing *Ctrl+Alt+I* to answer the questions!
-   use visual mode or **render** your file to produce a version that
    you can see!
-   **render** your file to make sure it runs (and that you haven't been
    working out of order)
-   save your work often
    -   **commit** it via git!
    -   **push** updates to github

## Overview

This practice reviews the [Compare means among groups
lecture](https://jsgosnell.github.io/cuny_biostats_book/content/chapters/Compare_means_among_populations.html){target="\"_blank"}.

## Examples

We will run ANOVA's using the *lm* function to connect them to other
test. First, build the model

```{r}
iris_anova <- lm(Sepal.Length~Species, iris)
```

Then use the object it created to test assumptions

```{r}
par(mfrow = c(2,2))
plot(iris_anova)
```

If assumptions are met, check the p-value using the *summary* or *Anova*
function.

```{r}
summary(iris_anova)
library(car)
Anova(iris_anova, type = "III")
```

If the overall test is significant, carry out post hoc tests (Tukey
shown here for all pairs, as most common)

```{r}
library(multcomp)
compare_cont_tukey <- glht(iris_anova, linfct = mcp(Species = "Tukey"))
summary(compare_cont_tukey)
```

Note a special case of an ANOVA (which is a special case of a linear
model) is when we only are comparing 2 populations. When this happens,
we have a t-test. For example, we can compare only *I. virginica* and
*I. setosa*.

```{r}
not_versicolor <- iris[iris$Species != "versicolor",]
t.test(Sepal.Length ~ Species, not_versicolor)
```

If you compare lm and ANOVA output, it may look slightly different.

```{r}
summary(lm(Sepal.Length ~ Species, not_versicolor))
```

That's because by default the t-test does not assume equal variances. We
can fix that.

```{r}
t.test(Sepal.Length ~ Species, not_versicolor, var.equal=T)
```

but in this case it has minimal impact.

If assumptions are not met, we can use the Kruskal Wallis non-parametric
test and associated post hoc tests.

```{r}
kruskal.test(Sepal.Length ~ Species, data = iris)
pairwise.wilcox.test(iris$Sepal.Length, 
                          iris$Species, 
                          p.adjust.method="holm")
```

or a bootstrap alternative

```{r}
library(WRS2)
t1waybt(Sepal.Length~Species, iris)
bootstrap_post_hoc <- mcppb20(Sepal.Length~Species, iris)
p.adjust(as.numeric(bootstrap_post_hoc$comp[,6]), "holm")
```

For 2 groups, the *boot.t.test* function in the **MKinfer** package is
also an option.

```{r}
library(MKinfer)
boot.t.test(Sepal.Length ~ Species, not_versicolor)
```

A final option (shown here for 2 groups) is to use permutation tests.
These tests work by rearranging the data in all (theoretically) possible
ways and calculating signals for each permutation. You can then compare
what we actually found to that to obtain p-values. Often times we can't
find all the combinations there are too many!), but a good sample
(10,000+) should work. The function that we'll use for this is
independence_test from the coin package. It uses the same arguments as
the t-test (including the formula interface).

```{r}
library(coin)
independence_test(Sepal.Length ~ Species, not_versicolor)
```

## Swirl lesson

Swirl is an R package that provides guided lessons to help you learn and
review material. These lessons should serve as a bridge between all the
code provided in the slides and background reading and the key functions
and concepts from each lesson. A full course lesson (all lessons
combined) can also be downloaded using the following instructions.

**THIS IS ONE OF THE FEW TIMES I RECOMMEND WORKING DIRECTLY IN THE
CONSOLE! THERE IS NO NEED TO DEVELOP A SCRIPT FOR THESE INTERACTIVE
SESSIONS, THOUGH YOU CAN!**

-   install the "swirl" package

-   run the following code once on the computer to install a new course

    ```{r, eval=FALSE}
    library(swirl)
    install_course_github("jsgosnell", "JSG_swirl_lessons")
    ```

-   start swirl!

    ```{r, eval=F}
    swirl()
    ```

    -   swirl()

-   then follow the on-screen prompts to select the JSG_swirl_lessons
    course and the lessons you want

    -   Here we will focus on the **Compare means among groups** lesson

-   TIP: If you are seeing duplicate courses (or odd versions of each),
    you can clear all courses and then re-download the courses by

    -   exiting swirl using escape key or bye() function

        ```{r, eval=F}
        bye()
        ```

    -   uninstalling and reinstalling courses

        ```{r, eval=F}
        uninstall_all_courses()
        install_course_github("jsgosnell", "JSG_swirl_lessons")
        ```

    -   when you restart swirl with swirl(), you may need to select

        -   No. Let me start something new

## Just for practice

*For the following questions, you will use multiple methods to analyze a
single dataset. This is for practice and so you can compare! For actual
analysis you should only use the best method!*

### 1

Use the iris dataset in R to determine if petal length differs among
species. *Do this problem using the following methods*

-   *ANOVA*

-   *Kruskal-Wallis*

-   *bootstrapping*

*Make sure you can plot the data and carry out multiple comparison
methods as needed. Also be sure to understand the use of coefficients
and adjusted R^2^ values and where to find them.*

```{r}
#plot
library(Rmisc)

function_output <- summarySE(iris, measurevar="Petal.Length", groupvars =
                               c("Species"))
library(ggplot2)
ggplot(function_output, aes(x=Species, y=Petal.Length)) +
  geom_col(aes(fill=Species), size = 3) +
  geom_errorbar(aes(ymin=Petal.Length-ci, ymax=Petal.Length+ci), size=1.5) +
  ylab("Petal Length (cm)")+ggtitle("Petal Length of \n various iris species")+
  theme(axis.title.x = element_text(face="bold", size=28), 
        axis.title.y = element_text(face="bold", size=28), 
        axis.text.y  = element_text(size=20),
        axis.text.x  = element_text(size=20), 
        legend.text =element_text(size=20),
        legend.title = element_text(size=20, face="bold"),
        plot.title = element_text(hjust = 0.5, face="bold", size=32))

petal <- lm(Petal.Length ~ Species, iris)
plot(petal)
library(car)
Anova(petal, type = "III")
#compare to
summary(petal)
library(multcomp)
comp_cholest <- glht(petal, linfct = mcp(Species = "Tukey"))
summary(comp_cholest)

#kw approach
petal <- kruskal.test(Petal.Length ~ Species, iris)
pairwise.wilcox.test(iris$Sepal.Length, 
                          iris$Species, 
                          p.adjust.method="holm")

#bootstrap
library(WRS2)
t1waybt(Petal.Length~Species, iris)
bootstrap_post_hoc <- mcppb20(Petal.Length~Species, iris)
#use p.adjust to correct for FWER
p.adjust(as.numeric(bootstrap_post_hoc$comp[,6]), "holm")
```

*Answer: We used an ANOVA (a special case of linear models) to
investigate how a numerical response variable differed among 3 groups.
This was appropriate as evidenced by the residual plots (there is no
pattern in the residuals and they are normally distributed), but other
methods are demonstrated as well.*

*Using an ANOVA, we found F~2,147~= 1180.2, which led to a p-value of
\<.001. Given this, I reject the null hypothesis there is no difference
among mean measurements for each species.\
Post-hoc testing indicated all species significantly differed from all
others (all p \<.05) using a Tukey approach to control for family-wise
error rate. Kruskal-Wallis and bootstrapping approaches led to similar
conclusions.*

### 2

Data on plant heights (in cm) for plants grown with a new and old
formulation of fertilizer can be found at

<https://docs.google.com/spreadsheets/d/e/2PACX-1vSUVowOKlmTic4ekL7LSbwDcqrsDSXv5K_c4Qyfcvz1lLE1_iINmGzy0zMGxY7z5DImlUErK4S2wY7Y/pub?gid=0&single=true&output=csv>.

Analyze this data using the t.test function and the lm function to
convince yourself that t-tests are special cases of ANOVAs, which are
special cases of linear models!

```{r}
fertilizer <- read.csv("https://docs.google.com/spreadsheets/d/e/2PACX-1vSUVowOKlmTic4ekL7LSbwDcqrsDSXv5K_c4Qyfcvz1lLE1_iINmGzy0zMGxY7z5DImlUErK4S2wY7Y/pub?gid=0&single=true&output=csv",
                       stringsAsFactors = T)
#note use of var.equal!  assumption of ANOVAs
t.test(height ~ fertilizer, fertilizer, var.equal = T)
fert_lm <- lm(height ~ fertilizer, fertilizer)
plot(fert_lm)
summary(fert_lm)
require(car)
Anova(fert_lm, type = "III")
```

*Answer: t-tests and ANOVA (lm) approaches yield the same results. Note
for the tests to match exactly we have to assume equal variances among
groups for the t-tests. In both we reject the null hypothesis of no
difference among mean height of plants based on fertilizer. Notice the t
statistic (2.9884) is the square root of the F statistic (8.931). The t
distribution corresponds to the F with only 1 df in the numerator (so
its not listed!).*

## For the following questions, pick the appropriate method for analyzing the question. Use a plot of the data and/or model analysis to justify your decision. Make sure you can carry out multiple comparison methods as needed. Also be sure to understand the use of coefficients and adjusted R^2^ values and where to find them.

### 3

Data on sugar cane yield for multiple fields is available using

read.table("<https://docs.google.com/spreadsheets/d/e/2PACX-1vRjstKreIM6UknyKFQCtw2_Q6itY9iOAVWO1hUNZkBFL8mwVssvTevqgzV22YDKCUeJq0HBDrsBrf5O/pub?gid=971470377&single=true&output=tsv>",
header = T, stringsAsFactors = T)

More info on the data can be found at
<http://www.statsci.org/data/oz/cane.html>. Is there evidence that
location (DistrictPosition column) impacts yield (Tonn.Hect column)? If
so, which areas are driving this distance?

```{r}
cane <- read.table("https://docs.google.com/spreadsheets/d/e/2PACX-1vRjstKreIM6UknyKFQCtw2_Q6itY9iOAVWO1hUNZkBFL8mwVssvTevqgzV22YDKCUeJq0HBDrsBrf5O/pub?gid=971470377&single=true&output=tsv", header = T, stringsAsFactors = T)
summary(cane)
cane_summary <- summarySE(cane, measurevar="Tonn.Hect", groupvars =
                               c("DistrictPosition"))

ggplot(cane_summary, aes(x=DistrictPosition, y=Tonn.Hect)) +
  geom_col(size = 3) +
  geom_errorbar(aes(ymin=Tonn.Hect-ci, ymax=Tonn.Hect+ci), size=1.5) +
  ylab("Production (tonnes per hectare)") +
  xlab("District Position") +
  ggtitle("Production differs \n among locations") +
  theme(axis.title.x = element_text(face="bold", size=28), 
        axis.title.y = element_text(face="bold", size=28), 
        axis.text.y  = element_text(size=20),
        axis.text.x  = element_text(size=20), 
        legend.text =element_text(size=20),
        legend.title = element_text(size=20, face="bold"),
        plot.title = element_text(hjust = 0.5, face="bold", size=32))

impact_district <- lm(Tonn.Hect ~ DistrictPosition, cane)
summary(impact_district)
plot(impact_district)#not really normal...lets bootstrap
require(WRS2)
t1waybt(Tonn.Hect ~ DistrictPosition, cane)
mcppb20(Tonn.Hect ~ DistrictPosition, cane)
p <- mcppb20(Tonn.Hect ~ DistrictPosition, cane)
p.adjust(as.numeric(p$comp[,6]), "holm")


#compare to lm apporach
require(car)
Anova(impact_district, type = "III")
require(multcomp)
comp_district <- glht(impact_district, linfct = mcp(DistrictPosition = "Tukey"))
summary(comp_district)

```

*Answer: For this analysis I used a bootstrap approach as the residual
plots suggested a non-normal distribution. Analysis revealed a test
statistics of 16.52 and p-value of 0, so I reject the null hypothesis of
no difference among Districts. since I rejected the null hypothesis, I
have to use post-hoc tsts to determine which groups are different than
the others.\
Post-hoc tests reveal all district areas differ from each other except
for south and east, south and central, and east and central (using
sequential FDR to control for family-wise error rate.)*

*Note that a linear model does lead to slightly different findings
regarding which districts differ from which others.*

### 4

Data on FEV (forced expiratory volume), a measure of lung function, can
be found at

<http://www.statsci.org/data/general/fev.txt>

More information on the dataset is available at

<http://www.statsci.org/data/general/fev.html>.

Is there evidence that FEV depends on gender? If so, which gender has
the higher FEV score? How much variance does gender explain?

```{r}
fev <- read.table("http://www.statsci.org/data/general/fev.txt", header = T,
                  stringsAsFactors = T)
fev_summary <- summarySE(fev, measurevar="FEV", groupvars =
                               c("Sex"))

ggplot(fev_summary, aes(x=Sex, y=FEV)) +
  geom_col(size = 3) +
  geom_errorbar(aes(ymin=FEV-ci, ymax=FEV+ci), size=1.5) +
  ylab("FEV (liters)") +
  xlab("Sex") +
  ggtitle("FEV differs \n among males and females") +
  theme(axis.title.x = element_text(face="bold", size=28), 
        axis.title.y = element_text(face="bold", size=28), 
        axis.text.y  = element_text(size=20),
        axis.text.x  = element_text(size=20), 
        legend.text =element_text(size=20),
        legend.title = element_text(size=20, face="bold"),
        plot.title = element_text(hjust = 0.5, face="bold", size=32))


fev_gender <- lm(FEV ~ Sex, fev)
plot(fev_gender) #anova is fine
Anova(fev_gender, type = "III")
summary(fev_gender)
```

*I used an ANOVA (or linear model, or t-test, here, all the same since 2
groups!) to consider the impact of sex on FEV. This was appropriate as
evidenced by the residual plots (there is no pattern in the residuals
and they are normally distributed). Results indicate there is a
difference among sexes (F~1,652~ = 29.607, p\<.001). There is no need
for post-hoc tests here since there are only 2 groups being considered.\
Coefficients related to the groups (note female is replaced by intercept
here, and the SexMale coefficient is relative to that) indicates that
males have a higher FEV on average. Graphs also show this relationship.*

### 5

The following data are human blood clotting times (in minutes) of
individuals given one of two different drugs.

| Drug B | Drug G |
|:------:|:------:|
|  8.8   |  9.9   |
|  8.4   |  9.0   |
|  7.9   |  11.1  |
|  8.7   |  9.6   |
|  9.1   |  8.7   |
|  9.6   |  10.4  |
|        |  9.5   |

Test the hypothesis that the mean clotting times are equal for the two
groups

-   Estimating the variance from the data
-   Using rank transform analysis
-   Using a permutation test
-   Using a bootstrap test

Test the hypothesis that the mean clotting times are equal for the two
groups

-   Estimating the variance from the data
-   Using rank transform analysis
-   Using a permutation test
-   Using a bootstrap test

```{r}
drug_b <- c( 8.8, 8.4, 7.9, 8.7, 9.1, 9.6)
drug_g <- c(9.9, 9.0, 11.1, 9.6, 8.7, 10.4, 9.5)
t.test(drug_b, drug_g)
```

*Using a un-paired t-test (we learn about paired tests in the next
chapter), since the experimental units were not matched and I assumed
the means of each group would follow a normal distribution of unknown
variance, I found a test statistics of t~10.701~=-2.544. This
corresponds to a p-value of 0.02. This p-value is \<.05, so I reject the
null hypothesis that the mean clotting times are the same for the two
drugs.*

*I could also do this using a linear model approach,but that needs a
long dataframe*

```{r}
drug_df <- data.frame(drug= c(rep("b", length(drug_b)), rep("g", length(drug_g))), time = c(drug_b, drug_g))
drug_lm <- lm(time~drug, drug_df)
plot(drug_lm)
Anova(drug_lm, type="III")
```

*but that assumes variances are equal*

```{r}
t.test(drug_b, drug_g,var.equal = T)
```

-   Using rank transform analysis

```{r}
wilcox.test(drug_b, drug_g)
```

*Using a un-paired rank-based test, which is appropriate when normality
assumptions can't be met and I assumed the means of each group would
follow a similar distribution, I found a test statistics of W=7. This
corresponds to a p-value of 0.05. This p-value is \>.05, so I fail to
reject the null hypothesis that the mean clotting times are the same for
the two drugs. This is the same as*

```{r}
kruskal.test(time~drug, drug_df)
```

*If we remove the continuity correction*

```{r}
wilcox.test(drug_b, drug_g, correct = F)
```

-   Using a permutation test

```{r}
require(coin) #requires data_frame
clotting <- data.frame(drug = c(rep("drug_b", length(drug_b)), rep("drug_g", 
                                                                   length(drug_g))),
                       clotting = c(drug_b, drug_g))
clotting$drug <- factor(clotting$drug)
independence_test(clotting ~ drug, clotting)
```

*Using a permutation test, which is not fully appropriate here due to
small sample sizes (and that also assumes similar distributions for each
group), I found a test statistics of Z=-2.0726.. This corresponds to a
p-value of 0.038. This p-value is \>.05, so I fail to reject the null
hypothesis that the mean clotting times are the same for the two drugs.*

-   Using a bootstrap test

```{r}
library(MKinfer)
boot.t.test(drug_b, drug_g)
```

*Using a bootstrap test with 10000 samples, which is not fully
appropriate here due to small sample sizes, I found a p value of 0.0047.
This p-value is \<.05, so I reject the null hypothesis that the mean
clotting times are the same for the two drugs.*

### 6

(Example from Handbook on Biological Statistics) Odd (stunted, short,
new) feathers were compared in color to typical feathers in Northern
Flickers (*Colaptes auratus*) [@wiebe2002] . Data is at

<https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/wiebe_2002_example.csv>

Test the hypothesis that odd and typical feathers did not differ using

-   a Student's t test and/or lm
-   a rank test
-   bootstrapping

Note we will return to this question next week!

-   a Student's t test and/or lm

```{r}
feather <-  read.csv("https://raw.githubusercontent.com/jsgosnell/CUNY-BioStats/master/datasets/wiebe_2002_example.csv", stringsAsFactors = T)
t.test(Color_index ~ Feather, data=feather)
```

*I assumed the difference in means was normally distributed given the
trait and sample size. The test resulted in a statistic of t~29.971~ =
-3.56. This corresponds to a p-value of .001. Since the p-value is
\<.05, I reject the null hypothesis that feather color is the same
between odd and typical feathers. Note if we assume the variances of the
groups are equal (not the default for a t-test)*

```{r}
t.test(Color_index ~ Feather, data=feather, var.equal=T)
```

*this equivalent too:*

```{r}
library(car)
Anova(lm(Color_index ~ Feather, data=feather), type="III")
```

*However (we'll get to this next chapter), we should really use a
blocking approach that accounts for the fact each bird was measured
twice. This can be a paired t-test (note you can no longer use paired
data in formula interface):*

```{r}
t.test(feather[feather$Feather == "Typical", "Color_index"],
       feather[feather$Feather == "Odd", "Color_index"],
       paired=TRUE)
```

*I used a paired t-test because feathers were measured on the same
bird.\
I also assumed the difference in means was normally distributed given
the trait and sample size. The test resulted in a statistic of t~15~ =
-4.06. This corresponds to a p-value of .001. Since the p-value is
\<.05, I reject the null hypothesis that feather color is the same
between odd and typical feathers. Note this equivalent too:*

```{r}
library(car)
Anova(lm(Color_index ~ Feather+Bird, data=feather))
```

-   a rank test

*I could used a rank-based test if I assumed the distribution of means
was symmetrical but normal.*

```{r}
wilcox.test(Color_index ~ Feather, data=feather)
```

*which led to W=45.5, p=0.002, thus I reject the null hypothesis.
However, as noted above, we will see next week we should use a paired
test.*

```{r}
wilcox.test(feather[feather$Feather == "Typical", "Color_index"],
       feather[feather$Feather == "Odd", "Color_index"],
       paired=TRUE)
```

*I used a paired rank-based test because feathers were measured on the
same bird. I did not assume the difference in means was normally
distributed but did assume it followed a symmetric distribution. The
test resulted in a statistic of V = 10. This corresponds to a p-value of
.001. Since the p-value is \<.05, I reject the null hypothesis that
feather color is the same between odd and typical feathers.*

-   a binary test is only possible for paired data (so not required
    here, but in case you are interested...)

```{r}
library(BSDA)
SIGN.test(feather[feather$Feather == "Odd", "Color_index"], 
          feather[feather$Feather == "Typical", "Color_index"])
```

*I used a sign test (always paired!) because feathers were measured on
the same bird. I did not assume the difference in means was normally
distributed or that the differences followed a symmetric distribution.
The test resulted in a statistic of s = 3. This corresponds to a p-value
of .02. Since the p-value is \<.05, I reject the null hypothesis that
feather color is the same between odd and typical feathers.*

-   bootstrapping

*Could be done using a non-paired approach*

```{r}
library(MKinfer)
boot.t.test(Color_index ~ Feather, data=feather)
```

*or a paired approach (next chapter)*

```{r}
boot.t.test(Color_index ~ Feather, data=feather, paired=TRUE)
```

*The basic approach yield a p-value of .001, so I reject the null
hypothesis that feather color is the same between odd and typical
feathers.*

*Since feathers were measured on the same bird, I also could have used a
bootstrap (10,000 samples) focused on the difference in color. This
resulted in a p-value of \<.001. Since the p-value is \<.05, I reject
the null hypothesis that feather color is the same between odd and
typical feathers using this approach as well.*

### 7

Find an example of a one-way ANOVA from a research article that is
related to your research or a field of interest (hint: use Google
Scholar or another search engine to search for a keyword of interest
plus ANOVA). Make sure you understand the connections between the
methods, results, and graphs. Briefly answer the following questions

-   What was the dependent variable?
-   What were the independent variables?
-   Was there a difference in mean values among the groups? If so, how
    did the paper address or explain this finding.
